{
 "cells": [
  {
   "cell_type": "code",
   "id": "5e9a1959b8fab857",
   "metadata": {},
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================== Configuration (Modify as needed) ======================\n",
    "CAPACITY_DIVISOR = 1000.0  # Normalize capacity by dividing by 1000\n",
    "COEF_PVAL_MAX = None  # e.g., set to 0.05 to filter coefficients by significance\n",
    "PATH_CAP_ISO = Path(\"./tables/datacenter_sum.xlsx\")\n",
    "PATH_CAP_CITY = Path(\"./tables_city/city_dc.xlsx\")\n",
    "CITY_SHEET = None\n",
    "PSEUDO_ISO_FOR_CITY = \"CITY\"\n",
    "\n",
    "PATH_COEF = Path(\"./fitting_result/res_dk_results.xlsx\")\n",
    "NONISO_COEF_SHEET = \"sheet1\"\n",
    "\n",
    "PATH_AVG_PRICE = Path(\"./fitting_result/zone_price_diff_means.xlsx\")\n",
    "NONISO_PRICE_SHEET = \"sheet1\"\n",
    "\n",
    "TARGET_YEARS = [2025, 2030]\n",
    "OUT_XLSX = Path(\"./fitting_result/dc_price_impacts_2025_2030_by_iso.xlsx\")\n",
    "\n",
    "# ==== Route 2: 2030 New Capacity Table (Figure 1) ====\n",
    "PATH_ROUTE2_CAP = Path(\n",
    "    \"./fitting_result/path2.xlsx\"\n",
    ")  # <- Change to the path of your \"Figure 1\" Excel file\n",
    "ROUTE2_SHEET = None  # Specific sheet name if it's not the first one\n",
    "BASE_YEAR_ROUTE2 = 2025\n",
    "OUT_XLSX_ROUTE2 = Path(\n",
    "    \"./fitting_result/dc_price_impacts_2025_2030_by_iso_route2.xlsx\"\n",
    ")  # Output path for Route 2 results\n",
    "# ==============================================================================\n",
    "\n",
    "# ---------- Utility: Name normalization (remove spaces/punctuation, ignore case) ----------\n",
    "_norm_rx = re.compile(r\"[^\\w]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return _norm_rx.sub(\"\", str(s)).lower()\n",
    "\n",
    "\n",
    "def read_all_sheets(path: Path) -> Dict[str, pd.DataFrame]:\n",
    "    return pd.read_excel(path, sheet_name=None)\n",
    "\n",
    "\n",
    "def pick_year_column(df: pd.DataFrame) -> str:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    # Check for English and Chinese aliases\n",
    "    for key in [\"year\"]:\n",
    "        if key in cols_lower:\n",
    "            return cols_lower[key]\n",
    "    first = df.columns[0]\n",
    "    sample = pd.to_numeric(df[first], errors=\"coerce\")\n",
    "    if sample.dropna().between(1900, 2100).all():\n",
    "        return first\n",
    "    raise ValueError(\"Cannot identify year column, please check headers.\")\n",
    "\n",
    "\n",
    "def tidy_capacity_sheet(df: pd.DataFrame, iso_name: str,\n",
    "                        is_city_block: bool = False) -> pd.DataFrame:\n",
    "    ycol = pick_year_column(df)\n",
    "    df = df.copy()\n",
    "    df = df.loc[:, [ycol] + [c for c in df.columns if c != ycol]]\n",
    "    df = df[pd.to_numeric(df[ycol], errors=\"coerce\").between(2020, 2035)].copy()\n",
    "    value_cols = [c for c in df.columns if c != ycol]\n",
    "    m = df.melt(id_vars=[ycol], value_vars=value_cols,\n",
    "                var_name=\"region\", value_name=\"capacity\")\n",
    "    m.rename(columns={ycol: \"year\"}, inplace=True)\n",
    "    m[\"iso\"] = iso_name\n",
    "    m[\"region_norm\"] = m[\"region\"].map(norm_name)\n",
    "    m[\"capacity\"] = pd.to_numeric(m[\"capacity\"], errors=\"coerce\") / CAPACITY_DIVISOR\n",
    "    return m[[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"]]\n",
    "\n",
    "\n",
    "def load_iso_capacity(path: Path) -> pd.DataFrame:\n",
    "    book = read_all_sheets(path)\n",
    "    frames = []\n",
    "    for sheet, df in book.items():\n",
    "        if df is not None and not df.empty:\n",
    "            frames.append(tidy_capacity_sheet(df, iso_name=sheet))\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n",
    "        columns=[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def load_city_capacity(path: Path, sheet_name: Optional[str], pseudo_iso: str) -> pd.DataFrame:\n",
    "    book = read_all_sheets(path)\n",
    "    if sheet_name is None:\n",
    "        for name, df in book.items():\n",
    "            if df is not None and not df.empty:\n",
    "                sheet_name = name\n",
    "                break\n",
    "    if sheet_name is None or sheet_name not in book:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"])\n",
    "    df = book[sheet_name]\n",
    "    return tidy_capacity_sheet(df, iso_name=pseudo_iso, is_city_block=True)\n",
    "\n",
    "\n",
    "# ==== Route 2: Read \"New Capacity\" table, only 2030 increments ====\n",
    "def load_route2_increments(path: Path,\n",
    "                           sheet_name: Optional[str],\n",
    "                           pseudo_iso: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the table (from your screenshot), extracts:\n",
    "    - Area  -> iso\n",
    "    - Load Zone / City -> region\n",
    "    - New Capacity -> inc_capacity (Increment in 2030 compared to 2025)\n",
    "    Returns columns: ['iso', 'region', 'region_norm', 'inc_capacity']\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"inc_capacity\"])\n",
    "\n",
    "    book = pd.read_excel(path, sheet_name=None)\n",
    "    if sheet_name is None:\n",
    "        for name, df in book.items():\n",
    "            if df is not None and not df.empty:\n",
    "                sheet_name = name\n",
    "                break\n",
    "    if sheet_name is None or sheet_name not in book:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"inc_capacity\"])\n",
    "\n",
    "    df = book[sheet_name].copy()\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "\n",
    "    iso_col = cols.get(\"area\", df.columns[0])\n",
    "    reg_col = None\n",
    "    for k in [\"load zone / city\", \"load zone/city\", \"load zone\", \"city\", \"zone\", \"region\"]:\n",
    "        if k in cols:\n",
    "            reg_col = cols[k]\n",
    "            break\n",
    "    if reg_col is None:\n",
    "        reg_col = df.columns[1]\n",
    "\n",
    "    inc_col = None\n",
    "    for k in [\"new capacity\", \"new_capacity\", \"capacity_new\", \"newcap\"]:\n",
    "        if k in cols:\n",
    "            inc_col = cols[k]\n",
    "            break\n",
    "    if inc_col is None:\n",
    "        inc_col = df.columns[-1]\n",
    "\n",
    "    out = df[[iso_col, reg_col, inc_col]].copy()\n",
    "    out.columns = [\"iso_raw\", \"region\", \"inc_capacity\"]\n",
    "\n",
    "    out[\"iso_raw\"] = out[\"iso_raw\"].astype(str).str.strip()\n",
    "    out[\"iso\"] = out[\"iso_raw\"].str.upper()\n",
    "    # Use pseudo-ISO for cities\n",
    "    out.loc[out[\"iso\"] == \"CITY\", \"iso\"] = pseudo_iso\n",
    "\n",
    "    out[\"region_norm\"] = out[\"region\"].map(norm_name)\n",
    "    out[\"inc_capacity\"] = pd.to_numeric(out[\"inc_capacity\"], errors=\"coerce\")\n",
    "    out = out[out[\"inc_capacity\"].notna()]\n",
    "\n",
    "    return out[[\"iso\", \"region\", \"region_norm\", \"inc_capacity\"]]\n",
    "\n",
    "\n",
    "# ==== Route 2: Convert \"2025 Base Capacity + New\" into new 2025/2030 capacities ====\n",
    "def build_route2_capacity(\n",
    "        cap_iso_long: pd.DataFrame,\n",
    "        cap_city_long: pd.DataFrame,\n",
    "        inc_long: pd.DataFrame,\n",
    "        base_year: int,\n",
    "        target_years: List[int],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    base_year: e.g., 2025\n",
    "    target_years: [2025, 2030], only uses the min and max years\n",
    "    Logic:\n",
    "      1. Take capacity from the base_year in original data (All ISO + CITY).\n",
    "      2. Outer merge with inc_long based on iso+region_norm.\n",
    "      3. Route 2 2030 capacity = base_year capacity + inc_capacity.\n",
    "         Route 2 base_year capacity = base_year capacity (unchanged).\n",
    "    \"\"\"\n",
    "    if inc_long is None or inc_long.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"])\n",
    "\n",
    "    base_all = pd.concat([cap_iso_long, cap_city_long], ignore_index=True)\n",
    "    base = base_all[base_all[\"year\"] == base_year].copy()\n",
    "\n",
    "    base = base[[\"iso\", \"region\", \"region_norm\", \"capacity\"]].rename(\n",
    "        columns={\"capacity\": \"capacity_base\"}\n",
    "    )\n",
    "\n",
    "    inc = inc_long[[\"iso\", \"region\", \"region_norm\", \"inc_capacity\"]].copy()\n",
    "\n",
    "    merged = base.merge(\n",
    "        inc,\n",
    "        on=[\"iso\", \"region_norm\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_base\", \"_inc\"),\n",
    "    )\n",
    "\n",
    "    # Prefer region name from base_year, otherwise use new table's\n",
    "    merged[\"region\"] = merged[\"region_base\"].fillna(merged[\"region_inc\"])\n",
    "\n",
    "    base_cap = pd.to_numeric(merged[\"capacity_base\"], errors=\"coerce\").fillna(0.0)\n",
    "    inc_cap = pd.to_numeric(merged[\"inc_capacity\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    year_lo = min(target_years)\n",
    "    year_hi = max(target_years)\n",
    "\n",
    "    cap_lo = merged[[\"iso\", \"region\", \"region_norm\"]].copy()\n",
    "    cap_lo[\"year\"] = year_lo\n",
    "    cap_lo[\"capacity\"] = base_cap\n",
    "\n",
    "    cap_hi = merged[[\"iso\", \"region\", \"region_norm\"]].copy()\n",
    "    cap_hi[\"year\"] = year_hi\n",
    "    cap_hi[\"capacity\"] = base_cap + inc_cap\n",
    "\n",
    "    out = pd.concat([cap_lo, cap_hi], ignore_index=True)\n",
    "    return out[[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"]]\n",
    "\n",
    "\n",
    "# --------- Read Coefficients (Supports \"Region-Coef table\" and \"Regression Result\" formats) ----------\n",
    "def _find_col(cols_map: Dict[str, str], candidates: List[str]) -> Optional[str]:\n",
    "    for k in candidates:\n",
    "        if k in cols_map:\n",
    "            return cols_map[k]\n",
    "    return None\n",
    "\n",
    "\n",
    "def tidy_coef_sheet(df: pd.DataFrame, iso_name: str) -> pd.DataFrame:\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"])\n",
    "\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "\n",
    "    # Possible column aliases\n",
    "    zone_col = _find_col(cols, [\"zone\", \"market area\", \"subzone\", \"region\"])\n",
    "    coef_col = _find_col(cols, [\"coef\", \"coefficient\", \"estimate\", \"beta\"])\n",
    "    pval_col = _find_col(cols, [\"pval\", \"p_value\", \"p-value\", \"p\"])\n",
    "\n",
    "    # Confidence interval column aliases\n",
    "    lo_col = _find_col(cols, [\"ci_lower\", \"ci lower\", \"lower\", \"lb\", \"lcl\", \"lci\", \"2.5%\", \"low\", \"lower_ci\"])\n",
    "    hi_col = _find_col(cols, [\"ci_upper\", \"ci upper\", \"upper\", \"ub\", \"ucl\", \"uci\", \"97.5%\", \"high\", \"upper_ci\"])\n",
    "\n",
    "    # -------- Structure A: Coefficients by Region --------\n",
    "    if zone_col is not None and coef_col is not None:\n",
    "        out = df[[zone_col, coef_col]].copy()\n",
    "        out.columns = [\"region\", \"coef\"]\n",
    "        out[\"ci_lower\"] = pd.to_numeric(df[lo_col], errors=\"coerce\") if lo_col else np.nan\n",
    "        out[\"ci_upper\"] = pd.to_numeric(df[hi_col], errors=\"coerce\") if hi_col else np.nan\n",
    "\n",
    "        # Optional significance filtering\n",
    "        if COEF_PVAL_MAX is not None and pval_col in df.columns:\n",
    "            mask = pd.to_numeric(df[pval_col], errors=\"coerce\") <= COEF_PVAL_MAX\n",
    "            out = out.loc[mask].copy()\n",
    "\n",
    "        out[\"iso\"] = iso_name\n",
    "        out[\"region_norm\"] = out[\"region\"].map(norm_name)\n",
    "        out[\"coef\"] = pd.to_numeric(out[\"coef\"], errors=\"coerce\")\n",
    "        return out[[\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"]]\n",
    "\n",
    "    # -------- Structure B: Regression Result (Variables in Rows), extract dc_local --------\n",
    "    if coef_col is None:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"])\n",
    "\n",
    "    # Variable Name Column: Prefer first column; use index if first column is numeric\n",
    "    var_col = df.columns[0]\n",
    "    if pd.api.types.is_numeric_dtype(df[var_col]):\n",
    "        var_series = pd.Index(df.index).astype(str)\n",
    "    else:\n",
    "        var_series = df[var_col].astype(str)\n",
    "\n",
    "    norm_var = lambda s: re.sub(r\"[\\W_]+\", \"\", str(s)).lower()\n",
    "    mask = var_series.map(norm_var).eq(norm_var(\"dc_local\"))\n",
    "    if not mask.any():\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"])\n",
    "\n",
    "    sub = df.loc[mask].copy()\n",
    "\n",
    "    # Significance filtering\n",
    "    if COEF_PVAL_MAX is not None and pval_col in sub.columns:\n",
    "        sub = sub[pd.to_numeric(sub[pval_col], errors=\"coerce\") <= COEF_PVAL_MAX]\n",
    "        if sub.empty:\n",
    "            return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"])\n",
    "\n",
    "    coef_val = pd.to_numeric(sub[coef_col], errors=\"coerce\").dropna()\n",
    "    ci_lo = pd.to_numeric(sub[lo_col], errors=\"coerce\").dropna() if lo_col else pd.Series(dtype=float)\n",
    "    ci_hi = pd.to_numeric(sub[hi_col], errors=\"coerce\").dropna() if hi_col else pd.Series(dtype=float)\n",
    "    if coef_val.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"iso\": [iso_name],\n",
    "        \"region\": [np.nan],\n",
    "        \"region_norm\": [np.nan],\n",
    "        \"coef\": [coef_val.iloc[0]],\n",
    "        \"ci_lower\": [ci_lo.iloc[0] if not ci_lo.empty else np.nan],\n",
    "        \"ci_upper\": [ci_hi.iloc[0] if not ci_hi.empty else np.nan],\n",
    "    })\n",
    "\n",
    "\n",
    "def load_all_coefs(path: Path, noniso_sheet: str, pseudo_iso: str) -> pd.DataFrame:\n",
    "    book = pd.read_excel(path, sheet_name=None)\n",
    "    frames = []\n",
    "    for sheet, df in book.items():\n",
    "        iso_name = pseudo_iso if sheet.lower() == noniso_sheet.lower() else sheet\n",
    "        frames.append(tidy_coef_sheet(df, iso_name))\n",
    "    frames = [f for f in frames if f is not None and not f.empty]\n",
    "    cols = [\"iso\", \"region\", \"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"]\n",
    "    return pd.concat(frames, ignore_index=True)[cols] if frames else pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "# ---------- Read Average Prices ----------\n",
    "def tidy_price_sheet(df: pd.DataFrame, iso_name: str) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"avg_price\"])\n",
    "    orig_map = {c.lower().strip(): c for c in df.columns}\n",
    "    zone_col = None\n",
    "    for k in [\"zone\",\"market area\", \"subzone\", \"region\"]:\n",
    "        if k in orig_map:\n",
    "            zone_col = orig_map[k]\n",
    "            break\n",
    "    if zone_col is None:\n",
    "        zone_col = df.columns[0]\n",
    "    candidates = [c for c in df.columns if c != zone_col]\n",
    "    num_cols = [c for c in candidates if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not num_cols:\n",
    "        can = []\n",
    "        for c in candidates:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                can.append(c)\n",
    "        num_cols = can\n",
    "    if not num_cols:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"avg_price\"])\n",
    "    price_col = num_cols[0]\n",
    "    out = df[[zone_col, price_col]].copy()\n",
    "    out.columns = [\"region\", \"avg_price\"]\n",
    "    out[\"iso\"] = iso_name\n",
    "    out[\"region_norm\"] = out[\"region\"].map(norm_name)\n",
    "    out[\"avg_price\"] = pd.to_numeric(out[\"avg_price\"], errors=\"coerce\")\n",
    "    return out[[\"iso\", \"region\", \"region_norm\", \"avg_price\"]]\n",
    "\n",
    "\n",
    "def load_all_avg_prices(path: Path, noniso_sheet: str, pseudo_iso: str) -> pd.DataFrame:\n",
    "    book = read_all_sheets(path)\n",
    "    frames = []\n",
    "    for sheet, df in book.items():\n",
    "        iso_name = pseudo_iso if sheet.lower() == noniso_sheet.lower() else sheet\n",
    "        frames.append(tidy_price_sheet(df, iso_name))\n",
    "    frames = [f for f in frames if f is not None and not f.empty]\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n",
    "        columns=[\"iso\", \"region\", \"region_norm\", \"avg_price\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- Main Calculation: Include Confidence Intervals ----------\n",
    "def compute_impacts_by_iso(\n",
    "        cap_iso_long: pd.DataFrame,\n",
    "        cap_city_long: pd.DataFrame,\n",
    "        coefs_long: pd.DataFrame,\n",
    "        price_long: pd.DataFrame,\n",
    "        target_years: List[int],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    caps = pd.concat([cap_iso_long, cap_city_long], ignore_index=True)\n",
    "    if caps.empty:\n",
    "        return {}\n",
    "\n",
    "    caps = caps[caps[\"year\"].isin(target_years)].copy()\n",
    "    result = {}\n",
    "\n",
    "    for iso, g in caps.groupby(\"iso\", sort=False):\n",
    "        wide_cap = (g.pivot_table(index=[\"region\", \"region_norm\"], columns=\"year\",\n",
    "                                  values=\"capacity\", aggfunc=\"first\")\n",
    "                    .reset_index())\n",
    "\n",
    "        coef_iso_all = coefs_long.loc[coefs_long[\"iso\"] == iso,\n",
    "        [\"region_norm\", \"coef\", \"ci_lower\", \"ci_upper\"]]\n",
    "        price_iso = price_long.loc[price_long[\"iso\"] == iso,\n",
    "        [\"region_norm\", \"avg_price\"]]\n",
    "\n",
    "        # Region-specific vs ISO default\n",
    "        coef_specific = coef_iso_all.loc[coef_iso_all[\"region_norm\"].notna()].copy()\n",
    "        iso_default = coef_iso_all.loc[coef_iso_all[\"region_norm\"].isna()].copy()\n",
    "\n",
    "        m = (wide_cap\n",
    "             .merge(coef_specific, on=\"region_norm\", how=\"left\")\n",
    "             .merge(price_iso, on=\"region_norm\", how=\"left\"))\n",
    "\n",
    "        # Fill missing with ISO default coefficients (including CIs)\n",
    "        if not iso_default.empty:\n",
    "            def _fill_with_default(col: str):\n",
    "                if col not in m.columns:  # Safety check\n",
    "                    m[col] = np.nan\n",
    "                default_val = pd.to_numeric(iso_default[col], errors=\"coerce\").dropna()\n",
    "                if not default_val.empty:\n",
    "                    m[col] = m[col].fillna(default_val.iloc[0])\n",
    "\n",
    "            for col in [\"coef\", \"ci_lower\", \"ci_upper\"]:\n",
    "                _fill_with_default(col)\n",
    "\n",
    "        # Calculate absolute and percentage increase for each year (with bounds)\n",
    "        for y in target_years:\n",
    "            inc_col = f\"inc_{y}\"\n",
    "            inc_lo = f\"inc_{y}_lo\"\n",
    "            inc_hi = f\"inc_{y}_hi\"\n",
    "            pct_col = f\"pct_{y}\"\n",
    "            pct_lo = f\"pct_{y}_lo\"\n",
    "            pct_hi = f\"pct_{y}_hi\"\n",
    "\n",
    "            if y in m.columns:\n",
    "                cap_y = pd.to_numeric(m[y], errors=\"coerce\")\n",
    "                coef = pd.to_numeric(m[\"coef\"], errors=\"coerce\")\n",
    "                lo = pd.to_numeric(m[\"ci_lower\"], errors=\"coerce\")\n",
    "                hi = pd.to_numeric(m[\"ci_upper\"], errors=\"coerce\")\n",
    "                price = pd.to_numeric(m[\"avg_price\"], errors=\"coerce\")\n",
    "\n",
    "                m[inc_col] = cap_y * coef\n",
    "                m[inc_lo] = cap_y * lo\n",
    "                m[inc_hi] = cap_y * hi\n",
    "\n",
    "                # Percentage (%); set to NaN if price<=0 or NaN\n",
    "                denom = price.replace(0, np.nan)\n",
    "                m[pct_col] = (m[inc_col] / denom) * 100.0\n",
    "                m[pct_lo] = (m[inc_lo] / denom) * 100.0\n",
    "                m[pct_hi] = (m[inc_hi] / denom) * 100.0\n",
    "            else:\n",
    "                for c in [inc_col, inc_lo, inc_hi, pct_col, pct_lo, pct_hi]:\n",
    "                    m[c] = np.nan\n",
    "\n",
    "        # Organize columns\n",
    "        base_cols = [\"region\", \"coef\", \"ci_lower\", \"ci_upper\", \"avg_price\"]\n",
    "        year_cols = []\n",
    "        for y in target_years:\n",
    "            if y in m.columns:\n",
    "                year_cols.append(y)  # Capacity column for verification\n",
    "            year_cols += [f\"inc_{y}_lo\", f\"inc_{y}\", f\"inc_{y}_hi\",\n",
    "                          f\"pct_{y}_lo\", f\"pct_{y}\", f\"pct_{y}_hi\"]\n",
    "\n",
    "        ordered = [c for c in base_cols + year_cols if c in m.columns]\n",
    "        result[iso] = m[ordered].copy()\n",
    "\n",
    "    return result\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# ========================= Run and Export =========================\n",
    "cap_iso = load_iso_capacity(PATH_CAP_ISO)\n",
    "cap_city = load_city_capacity(PATH_CAP_CITY, CITY_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "coefs = load_all_coefs(PATH_COEF, NONISO_COEF_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "avg_price = load_all_avg_prices(PATH_AVG_PRICE, NONISO_PRICE_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "\n",
    "# ------- Original Route -------\n",
    "by_iso = compute_impacts_by_iso(\n",
    "    cap_iso_long=cap_iso,\n",
    "    cap_city_long=cap_city,\n",
    "    coefs_long=coefs,\n",
    "    price_long=avg_price,\n",
    "    target_years=TARGET_YEARS\n",
    ")\n",
    "\n",
    "OUT_XLSX.parent.mkdir(parents=True, exist_ok=True)\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as w:\n",
    "    for iso, df in by_iso.items():\n",
    "        sheet = re.sub(r\"[^\\w\\-]\", \"_\", str(iso))[:31] or \"sheet\"\n",
    "        df.to_excel(w, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"✅ Original Route result generated: {OUT_XLSX}\")\n",
    "\n",
    "# ------- Route 2: 2025 + New Capacity -------\n",
    "route2_inc = load_route2_increments(PATH_ROUTE2_CAP, ROUTE2_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "if route2_inc is not None and not route2_inc.empty:\n",
    "    caps_route2 = build_route2_capacity(\n",
    "        cap_iso_long=cap_iso,\n",
    "        cap_city_long=cap_city,\n",
    "        inc_long=route2_inc,\n",
    "        base_year=BASE_YEAR_ROUTE2,\n",
    "        target_years=TARGET_YEARS,\n",
    "    )\n",
    "\n",
    "    cap_iso_r2 = caps_route2[caps_route2[\"iso\"] != PSEUDO_ISO_FOR_CITY].copy()\n",
    "    cap_city_r2 = caps_route2[caps_route2[\"iso\"] == PSEUDO_ISO_FOR_CITY].copy()\n",
    "\n",
    "    by_iso_r2 = compute_impacts_by_iso(\n",
    "        cap_iso_long=cap_iso_r2,\n",
    "        cap_city_long=cap_city_r2,\n",
    "        coefs_long=coefs,\n",
    "        price_long=avg_price,\n",
    "        target_years=TARGET_YEARS,\n",
    "    )\n",
    "\n",
    "    OUT_XLSX_ROUTE2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(OUT_XLSX_ROUTE2, engine=\"xlsxwriter\") as w:\n",
    "        for iso, df in by_iso_r2.items():\n",
    "            sheet = re.sub(r\"[^\\w\\-]\", \"_\", str(iso))[:31] or \"sheet\"\n",
    "            df.to_excel(w, sheet_name=sheet, index=False)\n",
    "\n",
    "    print(f\"✅ [Route 2] result generated: {OUT_XLSX_ROUTE2}\")\n",
    "else:\n",
    "    print(\"⚠️ Route 2 new capacity table not found, only calculated original route.\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybkb",
   "language": "python",
   "name": "pybkb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
