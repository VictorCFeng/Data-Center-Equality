{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import statsmodels.formula.api as smf\n",
    "import re\n",
    "\n",
    "# -------------------- Utility Functions --------------------\n",
    "def _open_xls(path: str | Path) -> pd.ExcelFile:\n",
    "    path = Path(path)\n",
    "    assert path.suffix.lower() in {\".xlsx\", \".xlsm\", \".xls\"}, f\"Excel file required: {path}\"\n",
    "    return pd.ExcelFile(path)\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return \"\".join(str(s).upper().replace(\"-\", \"\").replace(\"_\", \"\").split())\n",
    "\n",
    "def _pick_sheet(xls: pd.ExcelFile, iso: str) -> str:\n",
    "    target = _norm(iso)\n",
    "    for name in xls.sheet_names:\n",
    "        if _norm(name) == target:\n",
    "            return name\n",
    "    for name in xls.sheet_names:\n",
    "        if target in _norm(name):\n",
    "            return name\n",
    "    raise KeyError(f\"Sheet not found in {xls} for ISO: {iso}; available sheets: {xls.sheet_names}\")\n",
    "\n",
    "def _find_ts_col(cols) -> str:\n",
    "    keys = [\"timestamp\", \"time\", \"datetime\", \"date\", \"hour\", \"interval\"]\n",
    "    for c in cols:\n",
    "        lc = str(c).lower()\n",
    "        if any(k in lc for k in keys):\n",
    "            return c\n",
    "    return cols[0]  # Fallback: First column\n",
    "\n",
    "def _to_datetime(series: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        dt = pd.to_datetime(pd.to_numeric(series, errors=\"coerce\"),\n",
    "                            unit=\"d\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    return dt\n",
    "\n",
    "def _coerce_numeric(df: pd.DataFrame, except_cols=(\"ts\",)) -> pd.DataFrame:\n",
    "    num_cols = [c for c in df.columns if c not in except_cols]\n",
    "    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# -------------------- Table 1: Day-Ahead Prices (Hour x Load Zone, Wide Format) --------------------\n",
    "def read_table1_prices(path_excel: str | Path, iso: str = \"PJM\") -> pd.DataFrame:\n",
    "    xls = _open_xls(path_excel)\n",
    "    sheet = _pick_sheet(xls, iso)\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    ts_col = _find_ts_col(df.columns)\n",
    "    df = df.rename(columns={ts_col: \"ts\"})\n",
    "    df[\"ts\"] = _to_datetime(df[\"ts\"])\n",
    "    df = _coerce_numeric(df, except_cols=(\"ts\",)).dropna(subset=[\"ts\"])\n",
    "    out = df.melt(id_vars=\"ts\", var_name=\"zone\", value_name=\"lmp_da\")\n",
    "    out[\"iso\"] = iso\n",
    "    return out[[\"iso\", \"zone\", \"ts\", \"lmp_da\"]].sort_values([\"zone\", \"ts\"])\n",
    "\n",
    "# -------------------- Table 2: Temperature (Hour x Load Zone, Wide Format) --------------------\n",
    "def read_table2_temperature(path_excel: str | Path, iso: str = \"PJM\") -> pd.DataFrame:\n",
    "\n",
    "    xls = _open_xls(path_excel)\n",
    "    sheet = _pick_sheet(xls, iso)\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    ts_col = _find_ts_col(df.columns)\n",
    "    df = df.rename(columns={ts_col: \"ts\"}).copy()\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "    if df[\"ts\"].isna().all():\n",
    "        df[\"ts\"] = pd.to_datetime(pd.to_numeric(df[\"ts\"], errors=\"coerce\"),\n",
    "                                  unit=\"d\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    df = _coerce_numeric(df, except_cols=(\"ts\",)).dropna(subset=[\"ts\"])\n",
    "    out = df.melt(id_vars=\"ts\", var_name=\"zone\", value_name=\"temperature\")\n",
    "    out[\"iso\"] = iso\n",
    "\n",
    "    return out[[\"iso\", \"zone\", \"ts\", \"temperature\"]].sort_values([\"zone\", \"ts\"])\n",
    "\n",
    "# -------------------- Table 3: ISO Fuel Mix & Gas/Marginal Price (Hour x Fuel, Wide Format) --------------------\n",
    "def read_table3_fuelmix(path_excel: str | Path, iso: str = \"PJM\") -> pd.DataFrame:\n",
    "\n",
    "    strict_last = True\n",
    "    xls = _open_xls(path_excel)\n",
    "    sheet = _pick_sheet(xls, iso)\n",
    "    header = pd.read_excel(xls, sheet_name=sheet, nrows=0)\n",
    "    cols = list(header.columns)\n",
    "    ts_col = _find_ts_col(cols)\n",
    "    def _norm(s: str) -> str:\n",
    "        return str(s).strip().lower().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "    price_col = None\n",
    "    if not strict_last:\n",
    "        for c in cols[::-1]:  # Search from right to left, prefer the last occurrence\n",
    "            nc = _norm(c)\n",
    "            if \"gasmarginalprice\" in nc or \"marginalprice\" in nc:\n",
    "                price_col = c\n",
    "                break\n",
    "    if price_col is None:\n",
    "        price_col = cols[-1]  # Force use of last column\n",
    "\n",
    "    df = pd.read_excel(xls, sheet_name=sheet, usecols=[ts_col, price_col])\n",
    "    df.columns = [\"ts\", \"marginal_price\"]\n",
    "    df[\"ts\"] = _to_datetime(df[\"ts\"])\n",
    "    df[\"marginal_price\"] = pd.to_numeric(df[\"marginal_price\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"ts\"]).sort_values(\"ts\")\n",
    "    df[\"iso\"] = iso\n",
    "\n",
    "    return df[[\"iso\", \"ts\", \"marginal_price\"]]\n",
    "\n",
    "# -------------------- Table 4: Fuel Price (Supplementary) --------------------------\n",
    "def read_table4_fuel_price(path, iso):\n",
    "\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    df = df.rename(columns={df.columns[0]: \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    if iso not in df.columns:\n",
    "        raise ValueError(f\"ISO '{iso}' not found in headers, options: {df.columns[1:].tolist()}\")\n",
    "    out = df[[\"date\", iso]].rename(columns={iso: \"marginal_price\"}).sort_values(\"date\")\n",
    "    out[\"iso\"] = iso\n",
    "\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "# -------------------- Table 5: Data Center Cumulative Capacity (Year x Load Zone, Wide Format) --------------------\n",
    "def read_table5_dc_capacity(path_excel: str | Path, iso: str = \"PJM\") -> pd.DataFrame:\n",
    "\n",
    "    xls = _open_xls(path_excel)\n",
    "    sheet = _pick_sheet(xls, iso)\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    year_col = None\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower().startswith(\"year\"):\n",
    "            year_col = c; break\n",
    "    if year_col is None:\n",
    "        year_col = df.columns[0]\n",
    "    df = df.rename(columns={year_col: \"year\"})\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"year\"].ge(2020)].dropna(subset=[\"year\"]).copy()\n",
    "    zone_cols = [c for c in df.columns if c != \"year\"]\n",
    "    df[zone_cols] = df[zone_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    out = df.melt(id_vars=\"year\", var_name=\"zone\", value_name=\"dc_cum\").dropna(subset=[\"dc_cum\"])\n",
    "    out[\"iso\"] = iso\n",
    "\n",
    "    return out[[\"iso\",\"zone\",\"year\",\"dc_cum\"]].sort_values([\"zone\",\"year\"])\n",
    "\n",
    "# -------------------- Convenience Wrapper: Read all tables at once --------------------\n",
    "def load_all_for_iso(paths: dict, iso: str = \"PJM\") -> dict:\n",
    "    return { \"prices\":      read_table1_prices(paths[\"t1\"], iso=iso),\n",
    "             \"temperature\": read_table2_temperature(paths[\"t2\"], iso=iso),\n",
    "             \"fuelmix\":     read_table3_fuelmix(paths[\"t3\"], iso=iso),\n",
    "             \"dc_capacity\": read_table5_dc_capacity(paths[\"t5\"], iso=iso)}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a539f1878fe3d9bb",
   "metadata": {},
   "source": [
    "paths = { \"t1\": \"./tables/price.xlsx\",\n",
    "          \"t2\": \"./tables/temperature_filled.xlsx\",\n",
    "          \"t3\": \"./tables/fuel_marginal.xlsx\",\n",
    "          \"t5\": \"./tables/datacenter_sum.xlsx\"}\n",
    "data_hour = load_all_for_iso(paths, iso=\"CAISO\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3808c3d195f21aa",
   "metadata": {},
   "source": [
    "data = {}\n",
    "for key in ['prices', 'temperature']:\n",
    "    df = data_hour[key].copy()\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    df['date'] = df['ts'].dt.date\n",
    "    num_cols = df.select_dtypes(include='number').columns\n",
    "    daily_df = df.groupby(['iso', 'zone', 'date'])[num_cols].mean().reset_index()\n",
    "    data[key] = daily_df\n",
    "df = data_hour['fuelmix'].copy()\n",
    "df['date'] = pd.to_datetime(df['ts'])\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "fuelmix_daily = df.groupby(['iso', 'date'])[num_cols].mean().reset_index()\n",
    "data['fuelmix'] = fuelmix_daily\n",
    "data[\"fuelmix\"][\"marginal_price\"] = data[\"fuelmix\"][\"marginal_price\"].astype(float)\n",
    "data['dc_capacity'] = data_hour['dc_capacity'].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dca292432fe19ac",
   "metadata": {},
   "source": [
    "temp_df = data[\"temperature\"].copy()\n",
    "BASE_HEAT = 65.0\n",
    "BASE_COOL = 65.0\n",
    "HDD_SCALE = 100\n",
    "CDD_SCALE = 100\n",
    "T = pd.to_numeric(temp_df[\"temperature\"], errors=\"coerce\")\n",
    "hdd_raw = (BASE_HEAT - T).clip(lower=0)\n",
    "cdd_raw = (T - BASE_COOL).clip(lower=0)\n",
    "temp_df[\"HDD\"] = (hdd_raw / HDD_SCALE).astype(float)\n",
    "temp_df[\"CDD\"] = (cdd_raw / CDD_SCALE).astype(float)\n",
    "temp_df[\"HDD2\"] = temp_df[\"HDD\"] ** 2\n",
    "temp_df[\"CDD2\"] = temp_df[\"CDD\"] ** 2\n",
    "data[\"temperature\"] = temp_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2968b65cfadbef3c",
   "metadata": {},
   "source": [
    "price_df = data[\"prices\"].copy()\n",
    "price_df = price_df.sort_values([\"zone\",\"date\"])\n",
    "first_price = price_df.groupby(\"zone\")[\"lmp_da\"].transform(\"first\")\n",
    "price_df[\"price_diff\"] = price_df[\"lmp_da\"]\n",
    "data[\"prices\"] = price_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a1a3da84d8d2f50",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# %% 1) Select ISO, perform basic cleaning and merging\n",
    "iso_tag = \"CAISO\"\n",
    "prices = data[\"prices\"].query(\"iso == @iso_tag\").copy()\n",
    "temps  = data[\"temperature\"].query(\"iso == @iso_tag\")[[\"iso\",\"zone\",\"date\",\"temperature\",\"HDD\",\"CDD\",\"HDD2\",\"CDD2\" ]].copy()\n",
    "mix    = data[\"fuelmix\"].query(\"iso == @iso_tag\")[[\"iso\",\"date\",\"marginal_price\"]].copy()\n",
    "dc     = data[\"dc_capacity\"].query(\"iso == @iso_tag\").copy()\n",
    "# Time decomposition & month x hour fixed effect index (0~287)\n",
    "prices[\"date\"] = pd.to_datetime(prices[\"date\"], errors=\"coerce\")\n",
    "temps[\"date\"] = pd.to_datetime(temps[\"date\"], errors=\"coerce\")\n",
    "mix[\"date\"] = pd.to_datetime(mix[\"date\"], errors=\"coerce\")\n",
    "prices[\"year\"] = prices[\"date\"].dt.year\n",
    "prices[\"month\"] = prices[\"date\"].dt.month\n",
    "# Merge temperature and fuel marginal cost controls (X_{z,t})\n",
    "df = ( prices.merge(temps, on=[\"iso\",\"zone\",\"date\"], how=\"left\").merge(mix,on=[\"iso\",\"date\"],how=\"left\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e21f59cc88d3e8b6",
   "metadata": {},
   "source": [
    "# %% 2) Construct Data Center variables (Daily linear interpolation within year, reaching cumulative total at year-end)\n",
    "start_date = pd.Timestamp(\"2020-01-01\")\n",
    "end_date   = pd.Timestamp(\"2025-07-30\")\n",
    "df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)].copy()\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "keys_local = df[[\"iso\", \"zone\", \"year\"]].drop_duplicates()\n",
    "keys_iso   = df[[\"iso\", \"year\"]].drop_duplicates()\n",
    "dc_y = dc[(dc[\"year\"] >= 2020) & (dc[\"year\"] <= 2025)].copy()\n",
    "dc_local_raw = dc_y[[\"iso\", \"zone\", \"year\", \"dc_cum\"]].copy()\n",
    "dc_2020_25 = (keys_local.merge(dc_local_raw, on=[\"iso\",\"zone\",\"year\"], how=\"left\").sort_values([\"iso\",\"zone\",\"year\"]))\n",
    "\n",
    "# —— 1) Prepare annual \"Target (Year-end Cumulative)\" and \"Baseline (Previous Year-end Cumulative)\" —— #\n",
    "# Local: iso + zone dimension\n",
    "dc_local_year = (dc_2020_25.rename(columns={\"dc_cum\": \"dc_local_target\"})[[\"iso\", \"zone\", \"year\", \"dc_local_target\"]].sort_values([\"iso\",\"zone\",\"year\"]))\n",
    "dc_local_year[\"dc_local_prev\"] = (dc_local_year.groupby([\"iso\",\"zone\"])[\"dc_local_target\"].shift(1).fillna(0.0))\n",
    "dc_local_year = dc_local_year.fillna(0.0)\n",
    "# ISO Total: iso dimension\n",
    "dc_iso_year = (dc_2020_25.groupby([\"iso\",\"year\"], as_index=False)[\"dc_cum\"].sum().rename(columns={\"dc_cum\":\"dc_iso_target\"}).sort_values([\"iso\",\"year\"]))\n",
    "dc_iso_year[\"dc_iso_prev\"] = (dc_iso_year.groupby(\"iso\")[\"dc_iso_target\"].shift(1).fillna(0.0))\n",
    "dc_iso_year = dc_iso_year.fillna(0.0)\n",
    "\n",
    "# —— 2) Merge \"Target/Baseline\" into hourly dataframe —— #\n",
    "df = (df.merge(dc_local_year, on=[\"iso\",\"zone\",\"year\"], how=\"left\").merge(dc_iso_year,on=[\"iso\",\"year\"],how=\"left\"))\n",
    "for col in [\"dc_local_target\",\"dc_local_prev\",\"dc_iso_target\",\"dc_iso_prev\"]:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "# If target is missing for a year, keep same as previous (no growth)\n",
    "for cur_col, prev_col in [(\"dc_local_target\",\"dc_local_prev\"),(\"dc_iso_target\",\"dc_iso_prev\")]:\n",
    "    df[cur_col] = df[cur_col].fillna(df[prev_col])\n",
    "    df[prev_col] = df[prev_col].fillna(0.0)\n",
    "\n",
    "# —— 3) Calculate \"Intra-year Progress Coefficient\" (Jan 1 = 0, Dec 31 = 1; handles leap years) —— #\n",
    "year_start = pd.to_datetime(df[\"year\"].astype(str) + \"-01-01\")\n",
    "year_end   = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")\n",
    "# Use \"day\" as unit; normalize hourly data to date before calculating day index\n",
    "d0 = df[\"date\"].dt.normalize()\n",
    "elapsed_days = (d0 - year_start).dt.days.astype(\"int64\")            # 0,1,...,365/366-1\n",
    "total_days   = (year_end - year_start).dt.days.astype(\"int64\")      # 365 or 366\n",
    "frac = (elapsed_days / total_days).clip(lower=0.0, upper=1.0)       # [0,1]\n",
    "\n",
    "# —— 4) Linear interpolation within the year —— #\n",
    "df[\"dc_local\"]    = df[\"dc_local_prev\"] + (df[\"dc_local_target\"] - df[\"dc_local_prev\"]) * frac\n",
    "df[\"dc_iso_total\"] = df[\"dc_iso_prev\"]  + (df[\"dc_iso_target\"]  - df[\"dc_iso_prev\"])  * frac\n",
    "\n",
    "# —— 5) External volume (Other zones in same ISO) & Unit Conversion —— #\n",
    "df[\"dc_external\"] = (df[\"dc_iso_total\"] - df[\"dc_local\"]).clip(lower=0.0)\n",
    "# Keep float and apply 1/1000 conversion\n",
    "scale_cols1 = [\"dc_local\"]\n",
    "df[scale_cols1] = df[scale_cols1].astype(float) / 1000.0\n",
    "scale_cols2 = [\"dc_iso_total\", \"dc_external\"]\n",
    "df[scale_cols2] = df[scale_cols2].astype(float) / 1000.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e143e7f40e98ba32",
   "metadata": {},
   "source": [
    "# %% 3) Select columns for regression and perform minimal cleaning\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "df[\"dow\"] = df[\"date\"].dt.dayofweek\n",
    "need_cols = [\"price_diff\", \"dc_local\",\"dc_external\",\"dc_iso_total\", \"temperature\",'HDD','CDD','HDD2','CDD2', \"marginal_price\",\"zone\",\"month\",\"year\",\"day\",'dow','date' ]\n",
    "reg = df[need_cols].dropna().copy()\n",
    "print(f\"Sample size for regression: {len(reg):,} rows, Zones: {reg['zone'].nunique()}.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec622bf3cd83383",
   "metadata": {},
   "source": [
    "def winsorize_by_group(d, cols, ql=0.005, qh=0.995, by=\"zone\"):\n",
    "\n",
    "    g = d.groupby(by)\n",
    "    lo = g[cols].transform(lambda s: s.quantile(ql))\n",
    "    hi = g[cols].transform(lambda s: s.quantile(qh))\n",
    "    in_range = (d[cols] >= lo) & (d[cols] <= hi)\n",
    "    keep = in_range.all(axis=1)\n",
    "\n",
    "    return d.loc[keep].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d510a2d3535699a",
   "metadata": {},
   "source": [
    "THRESH = 0.000\n",
    "# —— 1) Zone Filtering: Exclude zones where dc_local < 0.01 throughout the period ——\n",
    "reg = reg.copy()\n",
    "reg[\"dc_local\"] = pd.to_numeric(reg[\"dc_local\"], errors=\"coerce\").fillna(0.0)\n",
    "zone_max = reg.groupby(\"zone\")[\"dc_local\"].apply(lambda s: s.abs().max())\n",
    "keep_zones = sorted(zone_max[zone_max > THRESH].index.tolist())\n",
    "drop_zones = sorted(zone_max[zone_max <= THRESH].index.tolist())\n",
    "#reg1 = reg.loc[reg[\"zone\"].isin(keep_zones)].copy()\n",
    "reg1 = winsorize_by_group(reg.loc[reg[\"zone\"].isin(keep_zones)].copy(), [\"price_diff\"], 0.05, 0.95, \"zone\")\n",
    "reg2 = winsorize_by_group(reg1, [\"marginal_price\"], 0.01, 0.99, \"zone\")\n",
    "\n",
    "zone_mean = pd.to_numeric(reg[\"price_diff\"], errors=\"coerce\").groupby(reg[\"zone\"]).mean()\n",
    "overall_mean_plus = float(zone_mean.mean())\n",
    "print(f\"Overall mean after zone aggregation: {overall_mean_plus:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4dd03afaf34cb370",
   "metadata": {},
   "source": [
    "# ===== Config: Change to your output directory and desired data table =====\n",
    "OUT_DIR = Path(\"./fitting_result\")       # Your output folder\n",
    "SRC = reg                               # Use winsorized result; change to 'reg' if raw data is preferred\n",
    "# ===== Calculate Mean per Zone =====\n",
    "zone_mean_df = (\n",
    "    pd.to_numeric(SRC[\"price_diff\"], errors=\"coerce\")\n",
    "      .groupby(SRC[\"zone\"]).mean()\n",
    "      .rename(\"price_diff_mean\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"zone\")\n",
    ")\n",
    "zone_mean_df[\"price_diff_mean\"] = zone_mean_df[\"price_diff_mean\"].round(6)\n",
    "# Try to get ISO name from data; default to \"ISO\"\n",
    "iso_name = iso_tag\n",
    "# ===== Write to Excel: Separate sheets by ISO in the same file =====\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_file = OUT_DIR / \"zone_price_diff_means.xlsx\"   # You can change the filename\n",
    "# ===== Write to Excel: Append/Replace sheet if exists; create new if not =====\n",
    "if out_file.exists():\n",
    "    with pd.ExcelWriter(out_file, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        zone_mean_df.to_excel(writer, sheet_name=str(iso_name), index=False)\n",
    "else:\n",
    "    with pd.ExcelWriter(out_file, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "        zone_mean_df.to_excel(writer, sheet_name=str(iso_name), index=False)\n",
    "\n",
    "print(f\"Written to {out_file} -> Sheet: {iso_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "705fe2152d416253",
   "metadata": {},
   "source": [
    "from linearmodels.panel import PanelOLS\n",
    "#\n",
    "reg2[\"date\"] = pd.to_datetime(reg1[\"date\"])   # Skip if already datetime\n",
    "panel = reg2.set_index([\"zone\", \"date\"]).sort_index()\n",
    "panel = panel.assign(zone=lambda d: d.index.get_level_values(\"zone\"))\n",
    "res_dk = PanelOLS.from_formula(\"price_diff ~ 1 + HDD2 + CDD2 + C(zone):marginal_price + dc_local + C(year) + C(month):C(dow) + EntityEffects\",\n",
    "    data=panel).fit(cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=14)\n",
    "print(res_dk.summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "353957547f3ca00b",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "def result_to_df(res):\n",
    "    df = pd.DataFrame({\n",
    "        \"coef\": res.params,\n",
    "        \"std_err\": res.std_errors,\n",
    "        \"t_stat\": res.tstats,\n",
    "        \"pval\": res.pvalues\n",
    "    })\n",
    "    ci = res.conf_int(level=0.95)\n",
    "    ci.columns = [\"ci_lower\", \"ci_upper\"]\n",
    "    return pd.concat([df, ci], axis=1)\n",
    "\n",
    "\n",
    "# Coefficients\n",
    "res_dk_df = result_to_df(res_dk)\n",
    "file_path = \"./fitting_result/res_dk_results.xlsx\"\n",
    "if not os.path.exists(file_path):\n",
    "    with pd.ExcelWriter(file_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        res_dk_df.to_excel(writer, sheet_name=iso_tag)\n",
    "else:\n",
    "    with pd.ExcelWriter(file_path, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "        res_dk_df.to_excel(writer, sheet_name=iso_tag)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybkb",
   "language": "python",
   "name": "pybkb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
