{
 "cells": [
  {
   "cell_type": "code",
   "id": "77b0f81b8a8b29a",
   "metadata": {},
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================== Configuration (Modify as needed) ======================\n",
    "CAPACITY_DIVISOR = 1000.0   # Normalize capacity by dividing by 1000\n",
    "COEF_PVAL_MAX = None        # e.g., set to 0.05 to filter coefficients by significance\n",
    "PATH_CAP_ISO   = Path(\"./tables/datacenter_sum.xlsx\")\n",
    "PATH_CAP_CITY  = Path(\"./tables_city/city_dc.xlsx\")\n",
    "CITY_SHEET     = None\n",
    "PSEUDO_ISO_FOR_CITY = \"CITY\"\n",
    "\n",
    "PATH_COEF      = Path(\"./fitting_result/res_dk_results.xlsx\")\n",
    "NONISO_COEF_SHEET = \"sheet1\"\n",
    "\n",
    "PATH_AVG_PRICE = Path(\"./fitting_result/zone_price_diff_means.xlsx\")\n",
    "NONISO_PRICE_SHEET = \"sheet1\"\n",
    "\n",
    "TARGET_YEARS = [2025, 2030]\n",
    "OUT_XLSX = Path(\"./fitting_result/dc_price_impacts_2025_2030_by_iso.xlsx\")\n",
    "# ==============================================================================\n",
    "\n",
    "# ---------- Utility: Name normalization (remove spaces/punctuation, ignore case) ----------\n",
    "_norm_rx = re.compile(r\"[^\\w]+\", flags=re.UNICODE)\n",
    "def norm_name(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return _norm_rx.sub(\"\", str(s)).lower()\n",
    "\n",
    "def read_all_sheets(path: Path) -> Dict[str, pd.DataFrame]:\n",
    "    return pd.read_excel(path, sheet_name=None)\n",
    "\n",
    "def pick_year_column(df: pd.DataFrame) -> str:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    # Check for English and Chinese aliases\n",
    "    for key in [\"year\"]:\n",
    "        if key in cols_lower:\n",
    "            return cols_lower[key]\n",
    "    first = df.columns[0]\n",
    "    sample = pd.to_numeric(df[first], errors=\"coerce\")\n",
    "    if sample.dropna().between(1900, 2100).all():\n",
    "        return first\n",
    "    raise ValueError(\"Cannot identify year column, please check headers.\")\n",
    "\n",
    "def tidy_capacity_sheet(df: pd.DataFrame, iso_name: str,\n",
    "                        is_city_block: bool = False) -> pd.DataFrame:\n",
    "    ycol = pick_year_column(df)\n",
    "    df = df.copy()\n",
    "    df = df.loc[:, [ycol] + [c for c in df.columns if c != ycol]]\n",
    "    df = df[pd.to_numeric(df[ycol], errors=\"coerce\").between(2020, 2035)].copy()\n",
    "    value_cols = [c for c in df.columns if c != ycol]\n",
    "    m = df.melt(id_vars=[ycol], value_vars=value_cols,\n",
    "                var_name=\"region\", value_name=\"capacity\")\n",
    "    m.rename(columns={ycol: \"year\"}, inplace=True)\n",
    "    m[\"iso\"] = iso_name\n",
    "    m[\"region_norm\"] = m[\"region\"].map(norm_name)\n",
    "    m[\"capacity\"] = pd.to_numeric(m[\"capacity\"], errors=\"coerce\") / CAPACITY_DIVISOR\n",
    "    return m[[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"]]\n",
    "\n",
    "def load_iso_capacity(path: Path) -> pd.DataFrame:\n",
    "    book = read_all_sheets(path)\n",
    "    frames = []\n",
    "    for sheet, df in book.items():\n",
    "        if df is not None and not df.empty:\n",
    "            frames.append(tidy_capacity_sheet(df, iso_name=sheet))\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n",
    "        columns=[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"]\n",
    "    )\n",
    "\n",
    "def load_city_capacity(path: Path, sheet_name: Optional[str], pseudo_iso: str) -> pd.DataFrame:\n",
    "    book = read_all_sheets(path)\n",
    "    if sheet_name is None:\n",
    "        for name, df in book.items():\n",
    "            if df is not None and not df.empty:\n",
    "                sheet_name = name\n",
    "                break\n",
    "    if sheet_name is None or sheet_name not in book:\n",
    "        return pd.DataFrame(columns=[\"iso\", \"region\", \"region_norm\", \"year\", \"capacity\"])\n",
    "    df = book[sheet_name]\n",
    "    return tidy_capacity_sheet(df, iso_name=pseudo_iso, is_city_block=True)\n",
    "\n",
    "# --------- Read Coefficients (Supports \"Region-Coef table\" and \"Regression Result\" formats) ----------\n",
    "def _find_col(cols_map: Dict[str, str], candidates: List[str]) -> Optional[str]:\n",
    "    for k in candidates:\n",
    "        if k in cols_map:\n",
    "            return cols_map[k]\n",
    "    return None\n",
    "\n",
    "def tidy_coef_sheet(df: pd.DataFrame, iso_name: str) -> pd.DataFrame:\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"])\n",
    "\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "\n",
    "    # Possible column aliases (English and Chinese)\n",
    "    zone_col = _find_col(cols, [\"zone\", \"region\", \"area\", \"market area\", \"subzone\"])\n",
    "    coef_col = _find_col(cols, [\"coef\", \"coefficient\", \"estimate\", \"beta\"])\n",
    "    pval_col = _find_col(cols, [\"pval\", \"p_value\", \"p-value\", \"p\"])\n",
    "\n",
    "    # Confidence Interval column aliases\n",
    "    lo_col = _find_col(cols, [\"ci_lower\", \"ci lower\", \"lower\", \"lb\", \"lcl\", \"lci\", \"2.5%\", \"low\", \"lower_ci\"])\n",
    "    hi_col = _find_col(cols, [\"ci_upper\", \"ci upper\", \"upper\", \"ub\", \"ucl\", \"uci\", \"97.5%\", \"high\", \"upper_ci\"])\n",
    "\n",
    "    # -------- Structure A: Coefficients by Region --------\n",
    "    if zone_col is not None and coef_col is not None:\n",
    "        out = df[[zone_col, coef_col]].copy()\n",
    "        out.columns = [\"region\", \"coef\"]\n",
    "        out[\"ci_lower\"] = pd.to_numeric(df[lo_col], errors=\"coerce\") if lo_col else np.nan\n",
    "        out[\"ci_upper\"] = pd.to_numeric(df[hi_col], errors=\"coerce\") if hi_col else np.nan\n",
    "\n",
    "        # Optional significance filtering\n",
    "        if COEF_PVAL_MAX is not None and pval_col in df.columns:\n",
    "            mask = pd.to_numeric(df[pval_col], errors=\"coerce\") <= COEF_PVAL_MAX\n",
    "            out = out.loc[mask].copy()\n",
    "\n",
    "        out[\"iso\"] = iso_name\n",
    "        out[\"region_norm\"] = out[\"region\"].map(norm_name)\n",
    "        out[\"coef\"] = pd.to_numeric(out[\"coef\"], errors=\"coerce\")\n",
    "        return out[[\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"]]\n",
    "\n",
    "    # -------- Structure B: Regression Result (Variables in Rows), extract dc_local --------\n",
    "    if coef_col is None:\n",
    "        return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"])\n",
    "\n",
    "    # Variable Name Column: Prefer first column; use index if first column is numeric\n",
    "    var_col = df.columns[0]\n",
    "    if pd.api.types.is_numeric_dtype(df[var_col]):\n",
    "        var_series = pd.Index(df.index).astype(str)\n",
    "    else:\n",
    "        var_series = df[var_col].astype(str)\n",
    "\n",
    "    norm_var = lambda s: re.sub(r\"[\\W_]+\",\"\",str(s)).lower()\n",
    "    mask = var_series.map(norm_var).eq(norm_var(\"dc_local\"))\n",
    "    if not mask.any():\n",
    "        return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"])\n",
    "\n",
    "    sub = df.loc[mask].copy()\n",
    "\n",
    "    # Significance Filtering\n",
    "    if COEF_PVAL_MAX is not None and pval_col in sub.columns:\n",
    "        sub = sub[pd.to_numeric(sub[pval_col], errors=\"coerce\") <= COEF_PVAL_MAX]\n",
    "        if sub.empty:\n",
    "            return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"])\n",
    "\n",
    "    coef_val = pd.to_numeric(sub[coef_col], errors=\"coerce\").dropna()\n",
    "    ci_lo = pd.to_numeric(sub[lo_col], errors=\"coerce\").dropna() if lo_col else pd.Series(dtype=float)\n",
    "    ci_hi = pd.to_numeric(sub[hi_col], errors=\"coerce\").dropna() if hi_col else pd.Series(dtype=float)\n",
    "    if coef_val.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"iso\": [iso_name],\n",
    "        \"region\": [np.nan],\n",
    "        \"region_norm\": [np.nan],\n",
    "        \"coef\": [coef_val.iloc[0]],\n",
    "        \"ci_lower\": [ci_lo.iloc[0] if not ci_lo.empty else np.nan],\n",
    "        \"ci_upper\": [ci_hi.iloc[0] if not ci_hi.empty else np.nan],\n",
    "    })\n",
    "\n",
    "def load_all_coefs(path: Path, noniso_sheet: str, pseudo_iso: str) -> pd.DataFrame:\n",
    "    book = pd.read_excel(path, sheet_name=None)\n",
    "    frames = []\n",
    "    for sheet, df in book.items():\n",
    "        iso_name = pseudo_iso if sheet.lower() == noniso_sheet.lower() else sheet\n",
    "        frames.append(tidy_coef_sheet(df, iso_name))\n",
    "    frames = [f for f in frames if f is not None and not f.empty]\n",
    "    cols = [\"iso\",\"region\",\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"]\n",
    "    return pd.concat(frames, ignore_index=True)[cols] if frames else pd.DataFrame(columns=cols)\n",
    "\n",
    "# ---------- Read Average Prices ----------\n",
    "def tidy_price_sheet(df: pd.DataFrame, iso_name: str) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"avg_price\"])\n",
    "    orig_map = {c.lower().strip(): c for c in df.columns}\n",
    "    zone_col = None\n",
    "    for k in [\"zone\", \"region\", \"area\", \"market area\", \"subzone\"]:\n",
    "        if k in orig_map:\n",
    "            zone_col = orig_map[k]\n",
    "            break\n",
    "    if zone_col is None:\n",
    "        zone_col = df.columns[0]\n",
    "    candidates = [c for c in df.columns if c != zone_col]\n",
    "    num_cols = [c for c in candidates if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not num_cols:\n",
    "        can = []\n",
    "        for c in candidates:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                can.append(c)\n",
    "        num_cols = can\n",
    "    if not num_cols:\n",
    "        return pd.DataFrame(columns=[\"iso\",\"region\",\"region_norm\",\"avg_price\"])\n",
    "    price_col = num_cols[0]\n",
    "    out = df[[zone_col, price_col]].copy()\n",
    "    out.columns = [\"region\",\"avg_price\"]\n",
    "    out[\"iso\"] = iso_name\n",
    "    out[\"region_norm\"] = out[\"region\"].map(norm_name)\n",
    "    out[\"avg_price\"] = pd.to_numeric(out[\"avg_price\"], errors=\"coerce\")\n",
    "    return out[[\"iso\",\"region\",\"region_norm\",\"avg_price\"]]\n",
    "\n",
    "def load_all_avg_prices(path: Path, noniso_sheet: str, pseudo_iso: str) -> pd.DataFrame:\n",
    "    book = read_all_sheets(path)\n",
    "    frames = []\n",
    "    for sheet, df in book.items():\n",
    "        iso_name = pseudo_iso if sheet.lower() == noniso_sheet.lower() else sheet\n",
    "        frames.append(tidy_price_sheet(df, iso_name))\n",
    "    frames = [f for f in frames if f is not None and not f.empty]\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n",
    "        columns=[\"iso\",\"region\",\"region_norm\",\"avg_price\"]\n",
    "    )\n",
    "\n",
    "# ---------- Main Calculation: Include Confidence Intervals ----------\n",
    "def compute_impacts_by_iso(\n",
    "    cap_iso_long: pd.DataFrame,\n",
    "    cap_city_long: pd.DataFrame,\n",
    "    coefs_long: pd.DataFrame,\n",
    "    price_long: pd.DataFrame,\n",
    "    target_years: List[int],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "\n",
    "    caps = pd.concat([cap_iso_long, cap_city_long], ignore_index=True)\n",
    "    if caps.empty:\n",
    "        return {}\n",
    "\n",
    "    caps = caps[caps[\"year\"].isin(target_years)].copy()\n",
    "    result = {}\n",
    "\n",
    "    for iso, g in caps.groupby(\"iso\", sort=False):\n",
    "        wide_cap = (g.pivot_table(index=[\"region\",\"region_norm\"], columns=\"year\",\n",
    "                                  values=\"capacity\", aggfunc=\"first\")\n",
    "                      .reset_index())\n",
    "\n",
    "        coef_iso_all = coefs_long.loc[coefs_long[\"iso\"] == iso,\n",
    "                                      [\"region_norm\",\"coef\",\"ci_lower\",\"ci_upper\"]]\n",
    "        price_iso = price_long.loc[price_long[\"iso\"] == iso,\n",
    "                                   [\"region_norm\",\"avg_price\"]]\n",
    "\n",
    "        # Region-specific vs ISO default\n",
    "        coef_specific = coef_iso_all.loc[coef_iso_all[\"region_norm\"].notna()].copy()\n",
    "        iso_default = coef_iso_all.loc[coef_iso_all[\"region_norm\"].isna()].copy()\n",
    "\n",
    "        m = (wide_cap\n",
    "             .merge(coef_specific, on=\"region_norm\", how=\"left\")\n",
    "             .merge(price_iso, on=\"region_norm\", how=\"left\"))\n",
    "\n",
    "        # Fill missing with ISO default coefficients (including CIs)\n",
    "        if not iso_default.empty:\n",
    "            def _fill_with_default(col: str):\n",
    "                if col not in m.columns:  # Safety check\n",
    "                    m[col] = np.nan\n",
    "                default_val = pd.to_numeric(iso_default[col], errors=\"coerce\").dropna()\n",
    "                if not default_val.empty:\n",
    "                    m[col] = m[col].fillna(default_val.iloc[0])\n",
    "\n",
    "            for col in [\"coef\",\"ci_lower\",\"ci_upper\"]:\n",
    "                _fill_with_default(col)\n",
    "\n",
    "        # Calculate absolute and percentage increase for each year (with bounds)\n",
    "        for y in target_years:\n",
    "            inc_col   = f\"inc_{y}\"\n",
    "            inc_lo    = f\"inc_{y}_lo\"\n",
    "            inc_hi    = f\"inc_{y}_hi\"\n",
    "            pct_col   = f\"pct_{y}\"\n",
    "            pct_lo    = f\"pct_{y}_lo\"\n",
    "            pct_hi    = f\"pct_{y}_hi\"\n",
    "\n",
    "            if y in m.columns:\n",
    "                cap_y = pd.to_numeric(m[y], errors=\"coerce\")\n",
    "                coef  = pd.to_numeric(m[\"coef\"], errors=\"coerce\")\n",
    "                lo    = pd.to_numeric(m[\"ci_lower\"], errors=\"coerce\")\n",
    "                hi    = pd.to_numeric(m[\"ci_upper\"], errors=\"coerce\")\n",
    "                price = pd.to_numeric(m[\"avg_price\"], errors=\"coerce\")\n",
    "\n",
    "                m[inc_col] = cap_y * coef\n",
    "                m[inc_lo]  = cap_y * lo\n",
    "                m[inc_hi]  = cap_y * hi\n",
    "\n",
    "                # Percentage (%); set to NaN if price<=0 or NaN\n",
    "                denom = price.replace(0, np.nan)\n",
    "                m[pct_col] = (m[inc_col] / denom) * 100.0\n",
    "                m[pct_lo]  = (m[inc_lo]  / denom) * 100.0\n",
    "                m[pct_hi]  = (m[inc_hi]  / denom) * 100.0\n",
    "            else:\n",
    "                for c in [inc_col, inc_lo, inc_hi, pct_col, pct_lo, pct_hi]:\n",
    "                    m[c] = np.nan\n",
    "\n",
    "        # Organize columns\n",
    "        base_cols = [\"region\", \"coef\", \"ci_lower\", \"ci_upper\", \"avg_price\"]\n",
    "        year_cols = []\n",
    "        for y in target_years:\n",
    "            if y in m.columns:\n",
    "                year_cols.append(y)  # Capacity column for verification\n",
    "            year_cols += [f\"inc_{y}_lo\", f\"inc_{y}\", f\"inc_{y}_hi\",\n",
    "                          f\"pct_{y}_lo\", f\"pct_{y}\", f\"pct_{y}_hi\"]\n",
    "\n",
    "        ordered = [c for c in base_cols + year_cols if c in m.columns]\n",
    "        result[iso] = m[ordered].copy()\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# ========================= Run and Export =========================\n",
    "cap_iso   = load_iso_capacity(PATH_CAP_ISO)\n",
    "cap_city  = load_city_capacity(PATH_CAP_CITY, CITY_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "coefs     = load_all_coefs(PATH_COEF, NONISO_COEF_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "avg_price = load_all_avg_prices(PATH_AVG_PRICE, NONISO_PRICE_SHEET, PSEUDO_ISO_FOR_CITY)\n",
    "\n",
    "by_iso = compute_impacts_by_iso(\n",
    "    cap_iso_long=cap_iso,\n",
    "    cap_city_long=cap_city,\n",
    "    coefs_long=coefs,\n",
    "    price_long=avg_price,\n",
    "    target_years=TARGET_YEARS\n",
    ")\n",
    "\n",
    "OUT_XLSX.parent.mkdir(parents=True, exist_ok=True)\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as w:\n",
    "    for iso, df in by_iso.items():\n",
    "        sheet = re.sub(r\"[^\\w\\-]\", \"_\", str(iso))[:31] or \"sheet\"\n",
    "        df.to_excel(w, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"âœ… Result generated: {OUT_XLSX}\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybkb",
   "language": "python",
   "name": "pybkb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
